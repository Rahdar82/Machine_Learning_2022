{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15623699-5cb2-4cde-a3c8-4f78875443b6",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "    <h1 align=\"center\">A Simple Neural Network</h1>\n",
    "    <h3 align=\"center\">Mohammad Rahdar</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a0c01-f990-49f7-8819-699f8d63b464",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Neural networks have revolutionized various fields, from image recognition to natural language processing. At their core, neural networks are mathematical models inspired by the human brain's neural structure. They consist of interconnected nodes, or neurons, organized into layers. Each neuron processes information and passes it on to the next layer, enabling the network to learn complex patterns and make predictions.\n",
    "\n",
    "A single-layer network without hidden layers is the simplest form of a neural network. This type of network is essentially performing **logistic regression**. It is a fundamental machine learning technique used for binary classification tasks, where the model predicts the probability of an instance belonging to a certain class (usually 0 or 1). Despite its name, logistic regression is not a neural network in the traditional sense but serves as a good starting point to understand the concepts underlying neural networks.\n",
    "\n",
    "## Key Concepts of Logistic Regression:\n",
    "\n",
    "1. **Linear Combination:** Logistic regression calculates a linear combination of the input features weighted by coefficients.\n",
    "\n",
    "2. **Activation Function:** Unlike other neural networks that use more complex activation functions, logistic regression uses the sigmoid function to squash the output into a probability score between 0 and 1.\n",
    "\n",
    "3. **Loss Function:** The model is trained using a loss function such as binary cross-entropy, which measures the difference between the predicted probabilities and the actual labels.\n",
    "\n",
    "4. **Optimization:** The training goal is to optimize the model's coefficients (weights) to minimize the loss function, typically using techniques like gradient descent.\n",
    "\n",
    "While logistic regression isn't a neural network with hidden layers, it forms the foundation upon which more complex neural architectures are built. Understanding its principles provides a solid starting point for delving into deeper neural network structures and their applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf7b90c-ffdf-46d3-83ae-99e72dc84446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e0fca-0fea-473b-8e22-35a590c118d7",
   "metadata": {},
   "source": [
    "# The Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "\n",
    "The Breast Cancer Wisconsin (Diagnostic) dataset is widely used in machine learning and medical research. It was originally collected at the University of Wisconsin Hospitals, Madison, USA, and donated to the UCI Machine Learning Repository.\n",
    "\n",
    "## Dataset Overview\n",
    "The dataset is designed for binary classification tasks where the goal is to predict whether a breast tumor is malignant (cancerous) or benign (non-cancerous). The dataset contains features computed from digitized images of fine needle aspirate (FNA) of breast mass. These features are computed from a digitized image of a breast mass and describe characteristics of the cell nuclei present in the image. The target variable, which is the variable we want to predict, indicates the diagnosis of the tumor. A diagnosis of \"M\" indicates malignant (cancerous), and \"B\" indicates benign (non-cancerous).\n",
    "\n",
    "Citation: <br>\n",
    "Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1995). Breast Cancer Wisconsin (Diagnostic) dataset. UCI Machine Learning Repository. Retrieved from https://doi.org/10.24432/C5DW2B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6988f8-9f68-4908-942d-e7ae470b1b69",
   "metadata": {},
   "source": [
    "The following Python code snippet fetches the Breast Cancer Wisconsin (Diagnostic) dataset from the UCI Machine Learning Repository using the fetch_ucirepo function. It retrieves the dataset's features (X) and targets (y) as Pandas DataFrames, making it ready for further analysis and model development.\n",
    "\n",
    "You may need to install `fetch_ucirepo` through `pip install ucimlrepo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6005be7-a283-47c8-800f-32f7a42c54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID  Categorical        None        None  None   \n",
      "1            Diagnosis   Target  Categorical        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature   Continuous        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "# Import the fetch_ucirepo function from the ucimlrepo module to\n",
    "# fetch datasets from the UCI Machine Learning Repository\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset\n",
    "# The id=17 parameter indicates which dataset to fetch\n",
    "breast_cancer = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer.data.features \n",
    "y = breast_cancer.data.targets \n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c364553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
       "1    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
       "2    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
       "3    11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
       "4    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
       "0      0.3001          0.14710     0.2419             0.07871  ...    25.38   \n",
       "1      0.0869          0.07017     0.1812             0.05667  ...    24.99   \n",
       "2      0.1974          0.12790     0.2069             0.05999  ...    23.57   \n",
       "3      0.2414          0.10520     0.2597             0.09744  ...    14.91   \n",
       "4      0.1980          0.10430     0.1809             0.05883  ...    22.54   \n",
       "\n",
       "   texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0     17.33      184.60  2019.0       0.1622        0.6656      0.7119   \n",
       "1     23.41      158.80  1956.0       0.1238        0.1866      0.2416   \n",
       "2     25.53      152.50  1709.0       0.1444        0.4245      0.4504   \n",
       "3     26.50       98.87   567.7       0.2098        0.8663      0.6869   \n",
       "4     16.67      152.20  1575.0       0.1374        0.2050      0.4000   \n",
       "\n",
       "   concave_points3  symmetry3  fractal_dimension3  \n",
       "0           0.2654     0.4601             0.11890  \n",
       "1           0.1860     0.2750             0.08902  \n",
       "2           0.2430     0.3613             0.08758  \n",
       "3           0.2575     0.6638             0.17300  \n",
       "4           0.1625     0.2364             0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695a7c26-7ee5-46b4-851f-73ec3e167230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Diagnosis\n",
       "0         M\n",
       "1         M\n",
       "2         M\n",
       "3         M\n",
       "4         M"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48259b7",
   "metadata": {},
   "source": [
    "Since the target column contains `M` and `B`, we need to use `map` function to manually map them to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474252de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert y from dataframe to a series\n",
    "y_series = y.squeeze()\n",
    "\n",
    "# Map 'M' to 0 and 'B' to 1\n",
    "y_mapped = y_series.map({'M': 0, 'B': 1})\n",
    "\n",
    "# Convert the Series back to a DataFrame\n",
    "y = y_mapped.to_frame(name='Diagnosis')\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37b4a7",
   "metadata": {},
   "source": [
    "It's generally considered best practice to split the data into training and test sets before scaling. This ensures that the scaling is based only on the training data, which helps to prevent data leakage. Data leakage can occur if information from the test set influences the scaling parameters, leading to overly optimistic performance estimates.\n",
    "\n",
    "Scaling after splitting ensures that the test set remains completely unseen during the training process. It simulates a real-world scenario where the model is applied to new, unseen data.\n",
    "\n",
    "Correct Procedure:\n",
    "- **Split the data**: Divide the dataset into training and test sets.\n",
    "- **Fit the scaler on the training data**: Compute the scaling parameters (mean and standard deviation) from the training data only.\n",
    "- **Transform both the training and test data**: Apply the same scaling parameters to both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7cac4f-0355-466e-8b92-c5023ea1528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a649b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002226f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled arrays back to DataFrames for easier manipulation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a37520-8398-488f-8da3-a4f465ceb92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training samples and features\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c899ca8c-dbf9-451d-9981-aa9544147abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 455)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose X\n",
    "X_train_scaled = X_train_scaled.T\n",
    "\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cf262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec668f59-29cd-4308-bf46-a659f5039021",
   "metadata": {},
   "source": [
    "# General Architecture\n",
    "\n",
    "Let's design a simple algorithm to predict the diagnosis of the tumor, malignant (cancerous) or benign (non-cancerous). We will build a Logistic Regression using a Neural Network mindset. The following Figure explains why Logistic Regression is a simple Neural Network!\n",
    "\n",
    "<center> <img src=\"LogReg_NN.png\" style=\"width:450px\">  </center>\n",
    "\n",
    "## Mathematical Expression\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b$$\n",
    "\n",
    "$$\\hat{y}^{(i)} = sigmoid(z^{(i)})$$ \n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(\\hat{y}^{(i)}) - (1-y^{(i)} )  \\log(1-\\hat{y}^{(i)})$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813b3f0-8267-4251-a64b-4a28d4e42d6f",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "Building a neural network involves several key steps, each crucial for creating a model to learn from data and make accurate predictions. Here are the main steps to build a neural network:\n",
    "\n",
    "1. **Define the Problem**    \n",
    "    Gather the dataset you'll use for training and testing. Ensure the data is cleaned and preprocessed (e.g., normalization, handling missing values).\n",
    "\n",
    "1. **Choose the Architecture**    \n",
    "    Decide whether to use a feedforward neural network, convolutional neural network (CNN), recurrent neural network (RNN), etc., based on the problem. Also, choose the number of layers and the number of neurons in each layer.\n",
    "\n",
    "1. **Initialize the Network**    \n",
    "    Start with random values for weights and biases. Proper initialization can help the network converge faster.\n",
    "\n",
    "1. **Set Up the Training Process**    \n",
    "    Choose the activation functions (ReLU, Sigmoid, Tanh, etc.) depending on the layer and the problem.    \n",
    "    Select the loss function (Mean Squared Error, Cross-Entropy) to measure the network's performance.    \n",
    "    Choose the optimization algorithm (Gradient Descent, Adam, RMSprop, etc.) to update the weights during training.    \n",
    "    Set the learning rate to determine the step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "1. **Train the Network**    \n",
    "    *Feedforward pass*: Pass the input data through the network to get the output.    \n",
    "    *Calculate the loss*: Compute the difference between the predicted and actual outputs using the loss function.    \n",
    "    *Backpropagation*: Calculate the gradient of the loss function for each weight using the chain rule and update the weights to minimize the loss.    \n",
    "    *Iterate over epochs*: Repeat several epochs' feedforward and backpropagation steps.\n",
    "\n",
    "1. **Evaluate the Network**    \n",
    "    Use a separate validation dataset to check how well the network generalizes to unseen data. Monitor metrics such as accuracy, precision, recall, F1 score, etc. Then, hyperparameters like learning rate, number of layers, number of neurons, etc., can be adjusted to improve performance.\n",
    "\n",
    "1. **Test the Network**    \n",
    "    After training and validation, test the final model on a separate test dataset to evaluate its performance. Check for overfitting, underfitting, and other issues. Ensure the model performs well on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d04d6e-04b3-45b4-b326-b63556f38cba",
   "metadata": {},
   "source": [
    "## Neural Network Structure for the Breast Cancer\n",
    "\n",
    "The main steps to build a very simple neural network for our classification problem are as follows:\n",
    "1. Define the model structure (such as the number of input features)\n",
    "1. Initialize the model's parameters\n",
    "1. Loop:\n",
    "    1. Calculate current loss (forward propagation)\n",
    "    1. Calculate current gradient (backward propagation)\n",
    "    1. Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b8528-5847-43b4-94d1-1496d195a9f7",
   "metadata": {},
   "source": [
    "### 1. Model Structure\n",
    "\n",
    "- The number of input features is 30.\n",
    "- There is no hidden layer for this network. So, we have one input layer with 32 neurons and one output layer with one neuron.\n",
    "- The activation function for the output layer is sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36fe12-acdf-4c5e-aa8e-473b18320825",
   "metadata": {},
   "source": [
    "### 2. Parameter Initialization\n",
    "\n",
    "Initializing weights to zero in a neural network is generally not recommended because it can lead to issues during training. When weights are initialized to zero, every neuron in the network receives the same gradient during backpropagation. This means that all neurons in each layer will update their weights in the same way, effectively learning the same features and not taking advantage of the neural network’s ability to learn diverse and complex features. Also, with zero initialization, the symmetry between neurons prevents the network from breaking the symmetry, leading to poor learning dynamics and a failure to converge to a good solution.\n",
    "\n",
    "To avoid the issues of zero initialization, several alternative initialization methods can be used:\n",
    "\n",
    "- Random Initialization: Initialize weights with small random values to break symmetry.    \n",
    "  - Uniform Distribution: Draw weights from a uniform distribution, e.g., `np.random.uniform(low, high, size)`.\n",
    "  - Normal Distribution: Draw weights from a normal distribution, e.g., `np.random.normal(mean, stddev, size)`.\n",
    "- Xavier (Glorot) Initialization\n",
    "- He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5997a91-888f-4421-a34a-9e8b3db38eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims, method='random_normal'):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a neural network.\n",
    "\n",
    "    Arguments:\n",
    "    layers_dims -- python list containing the dimensions of each layer in the network\n",
    "    method -- method to initialize the parameters: 'zeros', 'random_uniform', 'random_normal'\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing the parameters \"w1\", \"b1\", ..., \"wL\", \"bL\":\n",
    "                  wl -- weight matrix of shape (layers_dims[l-1], layers_dims[l])\n",
    "                  bl -- bias vector of shape (layers_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)  # for reproducibility\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        if method == 'zeros':\n",
    "            parameters['w' + str(l)] = np.zeros((layers_dims[l-1], layers_dims[l]))\n",
    "            parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        elif method == 'random_uniform':\n",
    "            parameters['w' + str(l)] = np.random.uniform(low=-0.01, high=0.01,\n",
    "                                                         size=(layers_dims[l-1], layers_dims[l]))\n",
    "            parameters['b' + str(l)] = np.random.uniform(low=-0.01, high=0.01,\n",
    "                                                         size=(layers_dims[l], 1))\n",
    "        elif method == 'random_normal':\n",
    "            parameters['w' + str(l)] = np.random.randn(layers_dims[l-1], layers_dims[l]) * 0.01\n",
    "            parameters['b' + str(l)] = np.random.randn(layers_dims[l], 1) * 0.01\n",
    "        else:\n",
    "            raise ValueError(\"Invalid initialization method: choose from 'zeros', 'random_uniform', or 'random_normal'\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f825004-7300-4c6b-a12b-31ddb76a0ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Initialize with zeros\n",
    "parameters = initialize_parameters(layers_dims=[30, 1], method='zeros')\n",
    "\n",
    "w = parameters['w1'].squeeze()\n",
    "b = parameters['b1']\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f1721-32dd-4051-b869-0ca85416dad5",
   "metadata": {},
   "source": [
    "### Forward and Backward Propagation\n",
    "\n",
    "After initialization, you can do the \"forward\" and \"backward\" propagation steps to learn the parameters.\n",
    "### 3.A. Forward Propagation\n",
    "Forward propagation is the process of passing input data through a neural network to generate an output. This involves the following steps:\n",
    "\n",
    "1. **Input Layer**: The input data is fed into the network.\n",
    "1. **Linear Transformation**: For each neuron in the network, a linear combination of the input values is computed using the weights and biases, where $w$ is the weight matrix, $X$ is the input data for the first layer, and $b$ is the bias vector.\n",
    "\n",
    "    $$ Z = w^T X + b $$\n",
    "   \n",
    "1. **Activation Function**: The linear combination is passed through an activation function (e.g., ReLU, Sigmoid, Tanh) to introduce non-linearity.\n",
    "\n",
    "   $$ A = \\sigma (Z) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) $$\n",
    "\n",
    "### 3.B. Backward Propagation\n",
    "Backward propagation is the process of updating the network's weights and biases to minimize the error between the predicted output and the actual target values. This involves the following steps:\n",
    "\n",
    "1. **Compute the Loss**: Calculate the loss (error) between the network's prediction and the actual target values using a loss function (e.g., Mean Squared Error, Cross-Entropy).\n",
    "   $$ J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})) $$\n",
    "\n",
    "1. **Calculate Gradients**:\n",
    "    $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)}) x^{(i)} = \\frac{1}{m}X(A-Y)^T$$\n",
    "    $$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a59b48-49b2-4783-9b54-51ec85a8d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation.\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a NumPy array of size (num_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_features, num_examples)\n",
    "    y -- true \"label\" vector containing 0 if malignant, or 1 if benign\n",
    "         of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    grads -- dictionary containing the gradients of the weights and bias\n",
    "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
    "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "\n",
    "    \"\"\"\n",
    "    # Number of examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propogation (from X to cost)\n",
    "    # Compute the linear combination of inputs and weights, and add the bias\n",
    "    z = np.dot(w.T, X) + b\n",
    "    \n",
    "    # Compute the activation using the sigmoid function\n",
    "    A = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Compute the cost using the logistic regression loss formula\n",
    "    cost = -1/m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))\n",
    "\n",
    "    # Backward propogation (to find gradients)\n",
    "    # Calculate the gradient of the loss with respect to w\n",
    "    dw = 1/m * np.dot(X, (A - y).T)\n",
    "    \n",
    "    # Calculate the gradient of the loss with respect to b\n",
    "    db = 1/m * np.sum(A - y)\n",
    "    \n",
    "    # Ensure cost is a scalar (use np.squeeze to remove any dimensions of size 1)\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    # Store gradients in a dictionary\n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0feb3a-ee63-4a28-b69e-cb61f955ce14",
   "metadata": {},
   "source": [
    "### 3.C. Optimization\n",
    "\n",
    "Update the weights and biases using the computed gradients and a chosen optimization algorithm (e.g., Gradient Descent, Adam).\n",
    "\n",
    "$$ w = w - \\alpha \\ dw $$\n",
    "$$ b = b - \\alpha \\ db $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1912f247-0298-4aa5-9913-8921321f5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, y, iter=100, alpha=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_features, num_examples)\n",
    "    y -- true \"label\" vector containing 0 if malignant, or 1 if benign\n",
    "         of size (1, number of examples)\n",
    "    iter -- number of iterations of the optimization loop\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store the cost at each iteration    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iter):\n",
    "        # Forward and backward propagation to get the gradients and cost \n",
    "        grads, cost = propagate(w, b, X, y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        # Update weights and bias using gradient descent\n",
    "        w = w - alpha*dw\n",
    "        b = b - alpha*db\n",
    "        \n",
    "        # Record the cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    # Store updated parameters in a dictionary\n",
    "    params = {'w': w, 'b': b}\n",
    "    \n",
    "    # Store final gradients in a dictionary\n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "338167f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (30,) and (569,30) not aligned: 30 (dim 0) != 569 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a82b781b5136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"w = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"b = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"dw = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dw\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-3d44f8262f0a>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(w, b, X, y, iter, alpha, print_cost)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Forward and backward propagation to get the gradients and cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Retrieve derivatives from grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-8088f2e43f66>\u001b[0m in \u001b[0;36mpropagate\u001b[1;34m(w, b, X, y)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Forward propogation (from X to cost)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Compute the linear combination of inputs and weights, and add the bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Compute the activation using the sigmoid function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (30,) and (569,30) not aligned: 30 (dim 0) != 569 (dim 0)"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, y)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b29620",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With the learned parameters $W$ and $b$, now we can predict the lable of a given sample. The prediction can be obtained babsed on the value of of activation in the last layer (a number between zero and one):\n",
    "$$ \\hat{y} = A = \\sigma(w^T X + b)$$\n",
    "\n",
    "If the actiation $\\leq$ 0.5, the prediction is malignant (cancerous) and if the actication $>$ 0.5, the prediction is benign (non-cancerous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b56bc75a-3490-478d-99ca-b22c0c28d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_features, num_examples)\n",
    "    \n",
    "    Returns:\n",
    "    y_pred -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    y_pred = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute the activation using the sigmoid function\n",
    "    z = np.dot(w.T,X) + b\n",
    "    A = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities to actual predictions\n",
    "        if A[0, i] > 0.5:\n",
    "            y_pred[0,i] = 1\n",
    "        else:\n",
    "            y_pred[0,i] = 0\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
